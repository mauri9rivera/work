{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import gpytorch\n",
    "import warnings\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.acquisition.analytic import LogExpectedImprovement, UpperConfidenceBound\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.models.utils import gpt_posterior_settings\n",
    "from botorch.models.transforms import Standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "import datetime\n",
    "from botorch.exceptions import OptimizationWarning, InputDataWarning, BadInitialCandidatesWarning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Adding modules\n",
    "sys.path.append(str(Path('./').resolve().parent.parent))\n",
    "from src.utils import synthetic_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OG paper conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Helper function to sample categorical data\n",
    "def sample_categorical(prob_partition, size):\n",
    "    return np.random.choice(len(prob_partition), size=size, p=prob_partition)\n",
    "\n",
    "# Main function\n",
    "def sample_struct_priors(xx, yy, fixhyp):\n",
    "    print(\"Start sample struct priors\")\n",
    "    dx = xx.shape[1]\n",
    "    n_partition = dx\n",
    "\n",
    "    hyp = {}\n",
    "\n",
    "    if all(k in fixhyp for k in [\"l\", \"sigma\", \"sigma0\"]):\n",
    "        hyp[\"l\"] = fixhyp[\"l\"]\n",
    "        hyp[\"sigma\"] = fixhyp[\"sigma\"]\n",
    "        hyp[\"sigma0\"] = fixhyp[\"sigma0\"]\n",
    "        decomp = learn_partition(xx, yy, hyp, fixhyp, n_partition)\n",
    "    else:\n",
    "        prob_partition = np.ones(n_partition) / n_partition\n",
    "        if \"z\" in fixhyp:\n",
    "            decomp = fixhyp[\"z\"]\n",
    "        else:\n",
    "            decomp = sample_categorical(prob_partition, dx)\n",
    "\n",
    "        num_iter = 2\n",
    "        guess_params = []\n",
    "\n",
    "        for _ in range(num_iter):\n",
    "            assert max(decomp) <= n_partition\n",
    "\n",
    "            def nll_func(params):\n",
    "                return compute_nlz_wrap(xx, yy, params, n_partition, decomp)\n",
    "\n",
    "            bounds = np.vstack([\n",
    "                np.tile([0, 10], (n_partition, 1)),\n",
    "                np.tile([-5, 2], (n_partition, 1)),\n",
    "                np.tile([-10, 1], (n_partition, 1))\n",
    "            ])\n",
    "\n",
    "            res = minimize(\n",
    "                nll_func, x0=np.zeros(bounds.shape[0]), bounds=bounds,\n",
    "                method='L-BFGS-B'\n",
    "            )\n",
    "\n",
    "            best_params = res.x\n",
    "            print(f\"Finished optimize hyp nll={res.fun}\")\n",
    "\n",
    "            guess_params.append(best_params)\n",
    "\n",
    "            l = np.exp(best_params[:n_partition][decomp])\n",
    "            sigma = np.exp(best_params[n_partition:2*n_partition])\n",
    "            sigma0 = np.exp(best_params[2*n_partition:])\n",
    "\n",
    "            hyp[\"l\"] = l\n",
    "            hyp[\"sigma\"] = sigma\n",
    "            hyp[\"sigma0\"] = sigma0\n",
    "\n",
    "            decomp = learn_partition(xx, yy, hyp, fixhyp, n_partition)\n",
    "            fixhyp[\"z\"] = decomp\n",
    "\n",
    "    return decomp, hyp\n",
    "\n",
    "# Helper function to learn the partition\n",
    "def learn_partition(xx, yy, hyp, fixhyp, n_partition):\n",
    "    if \"decomp\" in fixhyp:\n",
    "        return fixhyp[\"decomp\"]\n",
    "\n",
    "    N_gibbs = 10\n",
    "    gibbs_iter = N_gibbs // 2\n",
    "    dim_limit = 3\n",
    "    maxNdata = 750\n",
    "\n",
    "    Nidx = min(maxNdata, xx.shape[0])\n",
    "    xx = xx[:Nidx]\n",
    "    yy = yy[:Nidx]\n",
    "\n",
    "    hyp_dirichlet = np.ones(n_partition) * 1\n",
    "    prob_partition = hyp_dirichlet / hyp_dirichlet.sum()\n",
    "\n",
    "    if \"z\" in fixhyp:\n",
    "        z = fixhyp[\"z\"]\n",
    "    else:\n",
    "        z = sample_categorical(prob_partition, xx.shape[1])\n",
    "\n",
    "    z_best = None\n",
    "    minnlz = float('inf')\n",
    "\n",
    "    for i in range(N_gibbs):\n",
    "        for d in range(xx.shape[1]):\n",
    "            log_prob = np.full(n_partition, -np.inf)\n",
    "            nlz = np.full(n_partition, float('inf'))\n",
    "            \n",
    "            for a in range(n_partition):\n",
    "                z[d] = a\n",
    "\n",
    "                if i >= gibbs_iter and np.sum(z == a) >= dim_limit:\n",
    "                    continue\n",
    "\n",
    "                nlz[a] = compute_nlz(xx, yy, hyp, z)\n",
    "                log_prob[a] = np.log(np.sum(z == a) + hyp_dirichlet[a]) - nlz[a]\n",
    "\n",
    "            z[d] = np.argmax(log_prob - np.log(-np.log(np.random.rand(n_partition))))\n",
    "\n",
    "            if minnlz > nlz[z[d]]:\n",
    "                z_best = z.copy()\n",
    "                minnlz = nlz[z[d]]\n",
    "\n",
    "    return z_best\n",
    "\n",
    "# Placeholder for compute_nlz and compute_nlz_wrap\n",
    "def compute_nlz(xx, yy, hyp, z):\n",
    "    # Implement the calculation of the negative log likelihood here\n",
    "    return 0\n",
    "\n",
    "def compute_nlz_wrap(xx, yy, params, n_partition, decomp):\n",
    "    # Wrap the negative log likelihood computation\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gram(xx, hyp, hyp_idx, z):\n",
    "    \"\"\"\n",
    "    Compute the Gram matrix for an additive Gaussian process (add-GP).\n",
    "\n",
    "    Parameters:\n",
    "    - xx: numpy.ndarray\n",
    "        Input data of shape (n_samples, n_features).\n",
    "    - hyp: dict\n",
    "        Dictionary containing hyperparameters:\n",
    "        - 'l': numpy.ndarray of shape (n_partitions, n_features): length scales.\n",
    "        - 'sigma': numpy.ndarray of shape (n_partitions,): signal variances.\n",
    "        - 'sigma0': numpy.ndarray of shape (n_partitions,): noise variances.\n",
    "    - hyp_idx: int\n",
    "        Index for the hyperparameter set to use.\n",
    "    - z: numpy.ndarray\n",
    "        Array of shape (n_features,) defining the decomposition of input dimensions.\n",
    "        Each element specifies the partition index for the corresponding feature.\n",
    "\n",
    "    Returns:\n",
    "    - K: numpy.ndarray\n",
    "        Gram matrix of shape (n_samples, n_samples).\n",
    "    \"\"\"\n",
    "    all_cat = np.unique(z)  # Unique partition indices in z\n",
    "    K = 0  # Initialize Gram matrix\n",
    "\n",
    "    for category in all_cat:\n",
    "        # Get indices of features belonging to the current partition\n",
    "        feature_indices = np.where(z == category)[0]\n",
    "        \n",
    "        # Extract relevant data and hyperparameters\n",
    "        xx_partition = xx[:, feature_indices]\n",
    "        print(f'This is hyp: {hyp}')\n",
    "        l_partition = hyp['l'][hyp_idx, feature_indices]\n",
    "        sigma_partition = hyp['sigma'][hyp_idx, category]\n",
    "        sigma0_partition = hyp['sigma0'][hyp_idx, category]\n",
    "\n",
    "        # Compute the Gram matrix for this partition and accumulate\n",
    "        K += computeKmm(xx_partition, l_partition, sigma_partition, sigma0_partition)\n",
    "\n",
    "    return K\n",
    "\n",
    "def computeKmm(xx, l, sigma, sigma0):\n",
    "    \"\"\"\n",
    "    Compute the covariance (Gram) matrix for a single partition of features.\n",
    "\n",
    "    Parameters:\n",
    "    - xx: numpy.ndarray\n",
    "        Input data of shape (n_samples, n_features).\n",
    "    - l: numpy.ndarray\n",
    "        Length scales for the features.\n",
    "    - sigma: float\n",
    "        Signal variance.\n",
    "    - sigma0: float\n",
    "        Noise variance.\n",
    "\n",
    "    Returns:\n",
    "    - K: numpy.ndarray\n",
    "        Gram matrix of shape (n_samples, n_samples).\n",
    "    \"\"\"\n",
    "    # Compute squared distance matrix scaled by length scales\n",
    "    scaled_xx = xx / l\n",
    "    pairwise_sq_dists = np.sum(scaled_xx**2, axis=1, keepdims=True) - 2 * np.dot(scaled_xx, scaled_xx.T) + np.sum(scaled_xx**2, axis=1)\n",
    "\n",
    "    # Compute covariance matrix using the squared exponential kernel\n",
    "    K = sigma * np.exp(-0.5 * pairwise_sq_dists)\n",
    "\n",
    "    # Add noise variance on the diagonal\n",
    "    K += np.eye(K.shape[0]) * sigma0\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ucb_choose(xx, yy, kernel_matrix_inv, guesses, sigma0, sigma, l, xmin, xmax, alpha, beta):\n",
    "    \"\"\"\n",
    "    Select the next evaluation point using the Upper Confidence Bound (UCB) acquisition function.\n",
    "\n",
    "    Parameters:\n",
    "    - xx: numpy.ndarray\n",
    "        Observed input points of shape (n_samples, n_features).\n",
    "    - yy: numpy.ndarray\n",
    "        Observed output values of shape (n_samples,).\n",
    "    - kernel_matrix_inv: numpy.ndarray\n",
    "        Precomputed inverse Gram matrix for the Gaussian process.\n",
    "    - guesses: numpy.ndarray\n",
    "        Points to consider for evaluation, shape (n_guesses, n_features).\n",
    "    - sigma0: float\n",
    "        Noise variance.\n",
    "    - sigma: float\n",
    "        Signal variance.\n",
    "    - l: numpy.ndarray\n",
    "        Length scales of shape (n_features,).\n",
    "    - xmin: numpy.ndarray\n",
    "        Lower bounds of the search space, shape (n_features,).\n",
    "    - xmax: numpy.ndarray\n",
    "        Upper bounds of the search space, shape (n_features,).\n",
    "    - alpha: float\n",
    "        Exploration-exploitation balance parameter.\n",
    "    - beta: float\n",
    "        Scale factor for the confidence bound.\n",
    "\n",
    "    Returns:\n",
    "    - optimum: numpy.ndarray\n",
    "        The selected next evaluation point, shape (n_features,).\n",
    "    \"\"\"\n",
    "\n",
    "    def evaluate_ucb(x):\n",
    "        \"\"\"\n",
    "        Compute the UCB acquisition function value for a given point x.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: numpy.ndarray\n",
    "            A single input point of shape (n_features,).\n",
    "        \n",
    "        Returns:\n",
    "        - ucb: float\n",
    "            UCB acquisition function value.\n",
    "        \"\"\"\n",
    "        x = x.reshape(1, -1)  # Ensure x is 2D\n",
    "\n",
    "        # Compute kernel vector between x and observed data\n",
    "        k = compute_kernel_vector(x, xx, l, sigma)\n",
    "\n",
    "        # Predictive mean and variance\n",
    "        mu = k.T @ kernel_matrix_inv @ yy\n",
    "        sigma_sq = compute_kernel_scalar(x, l, sigma, sigma0) - k.T @ kernel_matrix_inv @ k\n",
    "\n",
    "        # UCB value\n",
    "        return -(mu + beta * np.sqrt(max(sigma_sq, 0)))  # Negative for maximization\n",
    "\n",
    "    # Use guesses and observed data as initial candidates\n",
    "    initial_points = np.vstack([guesses, xx])\n",
    "    best_ucb = float('-inf')\n",
    "    optimum = None\n",
    "\n",
    "    for x0 in initial_points:\n",
    "        # Constrain the optimization within bounds\n",
    "        bounds = [(xmin[i], xmax[i]) for i in range(len(xmin))]\n",
    "\n",
    "        # Optimize UCB starting from x0\n",
    "        res = minimize(evaluate_ucb, x0, bounds=bounds, method='L-BFGS-B')\n",
    "\n",
    "        if -res.fun > best_ucb:\n",
    "            best_ucb = -res.fun\n",
    "            optimum = res.x\n",
    "\n",
    "    return optimum\n",
    "\n",
    "\n",
    "def compute_kernel_vector(x, xx, l, sigma):\n",
    "    \"\"\"\n",
    "    Compute the kernel vector between a single point x and a dataset xx.\n",
    "\n",
    "    Parameters:\n",
    "    - x: numpy.ndarray\n",
    "        Single input point of shape (1, n_features).\n",
    "    - xx: numpy.ndarray\n",
    "        Dataset of shape (n_samples, n_features).\n",
    "    - l: numpy.ndarray\n",
    "        Length scales of shape (n_features,).\n",
    "    - sigma: float\n",
    "        Signal variance.\n",
    "\n",
    "    Returns:\n",
    "    - k: numpy.ndarray\n",
    "        Kernel vector of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    scaled_xx = xx / l\n",
    "    scaled_x = x / l\n",
    "    pairwise_sq_dists = np.sum(scaled_x**2, axis=1) - 2 * np.dot(scaled_x, scaled_xx.T) + np.sum(scaled_xx**2, axis=1)\n",
    "    return sigma * np.exp(-0.5 * pairwise_sq_dists)\n",
    "\n",
    "\n",
    "def compute_kernel_scalar(x, l, sigma, sigma0):\n",
    "    \"\"\"\n",
    "    Compute the kernel scalar (self-covariance) for a single point x.\n",
    "\n",
    "    Parameters:\n",
    "    - x: numpy.ndarray\n",
    "        Single input point of shape (1, n_features).\n",
    "    - l: numpy.ndarray\n",
    "        Length scales of shape (n_features,).\n",
    "    - sigma: float\n",
    "        Signal variance.\n",
    "    - sigma0: float\n",
    "        Noise variance.\n",
    "\n",
    "    Returns:\n",
    "    - k_scalar: float\n",
    "        Kernel scalar value.\n",
    "    \"\"\"\n",
    "    return sigma + sigma0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_gpopt(objective, xmin, xmax, T, initx=None, inity=None, options=None):\n",
    "    \"\"\"\n",
    "    Maximize the function objective using Bayesian Optimization with additive Gaussian Processes.\n",
    "\n",
    "    Parameters:\n",
    "    - objective: callable, the function to optimize.\n",
    "    - xmin, xmax: array-like, the bounds of the search space.\n",
    "    - T: int, the number of sequential evaluations.\n",
    "    - initx, inity: array-like, initial observed inputs and outputs.\n",
    "    - options: dict, additional options.\n",
    "\n",
    "    Returns:\n",
    "    - results: dict containing inferred argmax points, function values, evaluated points, and timing.\n",
    "    \"\"\"\n",
    "    # Default options\n",
    "    if options is None:\n",
    "        options = {}\n",
    "    options.setdefault('restart', 0)\n",
    "    options.setdefault('savefilenm', None)\n",
    "    options.setdefault('noiselevel', 0)\n",
    "    options.setdefault('nK', 1)\n",
    "    options.setdefault('nFeatures', 10000)\n",
    "    options.setdefault('seed', 42)\n",
    "    options.setdefault('learn_interval', 10)\n",
    "\n",
    "    np.random.seed(options['seed'])\n",
    "\n",
    "    if options['restart'] and options['savefilenm'] and os.path.exists(options['savefilenm']):\n",
    "        with open(options['savefilenm'], 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        xx = results['xx']\n",
    "        yy = results['yy']\n",
    "        choose_time = results['choose_time']\n",
    "        extra_time = results['extra_time']\n",
    "        t_start = results['t']\n",
    "        z = results['z']\n",
    "    else:\n",
    "        t_start = 0\n",
    "        if initx is None or inity is None:\n",
    "            initx = np.random.uniform(xmin, xmax, (1, len(xmin)))\n",
    "            inity = objective(initx)\n",
    "\n",
    "        xx = initx\n",
    "        yy = inity\n",
    "        choose_time = []\n",
    "        extra_time = []\n",
    "\n",
    "    for t in range(t_start + 1, T + 1):\n",
    "        # Learn structure\n",
    "        if t % options['learn_interval'] == 1:\n",
    "            start_time = time.time()\n",
    "            z, hyp = sample_struct_priors(xx, yy, options)\n",
    "            options['z'] = z\n",
    "            extra_time.append(time.time() - start_time)\n",
    "\n",
    "        # Choose next point\n",
    "        start_time = time.time()\n",
    "        kernel_matrix = compute_gram(xx, hyp, 1, z)\n",
    "        kernel_matrix_inv = np.linalg.inv(kernel_matrix)\n",
    "\n",
    "        x_next = np.zeros_like(xx[0])\n",
    "        all_categories = np.unique(z)\n",
    "\n",
    "        for cat in all_categories:\n",
    "            coords = z == cat\n",
    "            xx_sub = xx[:, coords]\n",
    "            xmin_sub = xmin[coords]\n",
    "            xmax_sub = xmax[coords]\n",
    "            l = hyp['l'][:, coords]\n",
    "            sigma = hyp['sigma'][cat]\n",
    "            sigma0 = hyp['sigma0'][cat]\n",
    "            alpha = 1\n",
    "            beta = np.sqrt(len(xx_sub[0]) * np.log(2 * t) / 5)\n",
    "\n",
    "            optimum = ucb_choose(xx_sub, yy, kernel_matrix_inv, sigma0, sigma, l, xmin_sub, xmax_sub, alpha, beta)\n",
    "            x_next[coords] = optimum\n",
    "\n",
    "        choose_time.append(time.time() - start_time)\n",
    "\n",
    "        xx = np.vstack([xx, x_next])\n",
    "        yy = np.vstack([yy, objective(x_next) + np.random.normal(0, options['noiselevel'], size=(1,))])\n",
    "\n",
    "        print(f\"{t}: val={yy[-1][0]}\")\n",
    "\n",
    "        if options['savefilenm'] and t % 10 == 0:\n",
    "            results = {\n",
    "                'xx': xx,\n",
    "                'yy': yy,\n",
    "                'choose_time': choose_time,\n",
    "                'extra_time': extra_time,\n",
    "                't': t,\n",
    "                'z': z\n",
    "            }\n",
    "            with open(options['savefilenm'], 'wb') as f:\n",
    "                pickle.dump(results, f)\n",
    "\n",
    "    return {\n",
    "        'xx': xx,\n",
    "        'yy': yy,\n",
    "        'choose_time': choose_time,\n",
    "        'extra_time': extra_time\n",
    "    }\n",
    "\n",
    "# Supporting functions (e.g., sample_struct_priors, compute_gram, ucb_choose) should also be implemented.\n",
    "# These functions involve translating the specific mathematical operations in MATLAB to Python using NumPy or SciPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define function to sample from additive Gaussian Process (add-GP)\n",
    "def sample_addGP(dx, n_samples, xmin, xmax):\n",
    "    \"\"\"\n",
    "    Placeholder for the sample_addGP function. This should sample a function\n",
    "    from an additive Gaussian Process.\n",
    "\n",
    "    Parameters:\n",
    "    - dx: int, number of dimensions.\n",
    "    - n_samples: int, number of samples to generate.\n",
    "    - xmin: numpy.ndarray, lower bounds of the input space.\n",
    "    - xmax: numpy.ndarray, upper bounds of the input space.\n",
    "\n",
    "    Returns:\n",
    "    - function: Callable that takes an input and evaluates the sampled add-GP.\n",
    "    \"\"\"\n",
    "    # Placeholder implementation (replace with your actual function)\n",
    "    def sampled_function(x):\n",
    "        return np.sum(np.sin(x), axis=-1)\n",
    "\n",
    "    return sampled_function\n",
    "\n",
    "\n",
    "# Define main script logic\n",
    "def main():\n",
    "    # Set parameters\n",
    "    dx = 20\n",
    "    xmin = np.zeros(dx)\n",
    "    xmax = np.ones(dx)\n",
    "\n",
    "    # Sample function from add-GP\n",
    "    f = sample_addGP(dx, dx, xmin, xmax)\n",
    "\n",
    "    # Set options\n",
    "    options = {\n",
    "        \"savefilenm\": None,  # Placeholder for save file name\n",
    "        \"nK\": 5,  # Number of maximums to sample\n",
    "        \"l\": np.ones(dx) * 50,  # Length scale hyperparameter\n",
    "        \"sigma\": np.ones(dx) * 5,  # Signal variance\n",
    "        \"sigma0\": np.ones(dx) * 0.0001,  # Noise variance\n",
    "    }\n",
    "\n",
    "    # Start Bayesian Optimization with Add-GP\n",
    "    add_gpopt(f, xmin, xmax, 200,options=options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel):\n",
    "        super(GP, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to sample categorical data\n",
    "def sample_categorical(prob_partition, size):\n",
    "    #e.g. sample_cateogircal([0.1, 0.1, 0.4, 0.2, 0.2], 4) = array([2, 2, 2, 3])\n",
    "    return np.random.choice(len(prob_partition), size=size, p=prob_partition)\n",
    "\n",
    "# Main function\n",
    "def sample_struct_priors(xx, yy, fixhyp):\n",
    "    print(\"Start sample struct priors\")\n",
    "    dx = xx.shape[1]\n",
    "    n_partition = dx\n",
    "\n",
    "    hyp = {}\n",
    "\n",
    "    if all(k in fixhyp for k in [\"l\", \"sigma\", \"sigma0\"]):\n",
    "        hyp[\"l\"] = fixhyp[\"l\"]\n",
    "        hyp[\"sigma\"] = fixhyp[\"sigma\"]\n",
    "        hyp[\"sigma0\"] = fixhyp[\"sigma0\"]\n",
    "        decomp = learn_partition(xx, yy, hyp, fixhyp, n_partition)\n",
    "    else:\n",
    "        prob_partition = np.ones(n_partition) / n_partition\n",
    "        decomp = fixhyp.get(\"z\", sample_categorical(prob_partition, dx))\n",
    "        \n",
    "        '''\n",
    "        num_iter = 2\n",
    "        for _ in range(num_iter):\n",
    "            assert max(decomp) <= n_partition\n",
    "\n",
    "            def compute_mll(params):\n",
    "                hyp = {\n",
    "                    \"l\": np.exp(params[:n_partition]),\n",
    "                    \"sigma\": np.exp(params[n_partition:2 * n_partition]),\n",
    "                    \"sigma0\": np.exp(params[2 * n_partition:])\n",
    "                }\n",
    "                return compute_nlz(xx, yy, hyp, decomp)\n",
    "\n",
    "            bounds = np.vstack([\n",
    "                np.tile([0, 10], (n_partition, 1)),\n",
    "                np.tile([-5, 2], (n_partition, 1)),\n",
    "                np.tile([-10, 1], (n_partition, 1))\n",
    "            ])\n",
    "\n",
    "            res = minimize(\n",
    "                compute_mll, x0=np.zeros(bounds.shape[0]), bounds=bounds,\n",
    "                method='L-BFGS-B'\n",
    "            )\n",
    "\n",
    "            best_params = res.x\n",
    "            #print(f\"Finished optimize hyp nll={res.fun}\")\n",
    "\n",
    "            l = np.exp(best_params[:n_partition][decomp])\n",
    "            sigma = np.exp(best_params[n_partition:2*n_partition])\n",
    "            sigma0 = np.exp(best_params[2*n_partition:])\n",
    "\n",
    "            hyp[\"l\"] = l\n",
    "            hyp[\"sigma\"] = sigma\n",
    "            hyp[\"sigma0\"] = sigma0\n",
    "\n",
    "            decomp = learn_partition(xx, yy, hyp, fixhyp, n_partition)\n",
    "            fixhyp[\"z\"] = decomp\n",
    "        '''\n",
    "\n",
    "        # GPytorch Model Training\n",
    "        #model = SingleTaskGP(xx, yy, outcome_transform=Standardize(m=1))\n",
    "        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        model = GP(xx, yy, likelihood, gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims= dx)))\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "        training_iter = 50\n",
    "        model.train()\n",
    "        likelihood.train()\n",
    "\n",
    "        for i in range(training_iter):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(model.train_inputs[0])\n",
    "            loss = -mll(output, model.train_targets).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Extract optimized hyperparameters\n",
    "        hyp[\"l\"] = model.covar_module.base_kernel.lengthscale.detach().numpy().squeeze()\n",
    "        hyp[\"sigma\"] = model.covar_module.outputscale.detach().numpy()\n",
    "        hyp[\"sigma0\"] = model.likelihood.noise.detach().numpy()\n",
    "\n",
    "        decomp = learn_partition(xx, yy, hyp, fixhyp, n_partition)\n",
    "        fixhyp[\"z\"] = decomp\n",
    "        \n",
    "\n",
    "    return decomp, hyp\n",
    "\n",
    "# Helper function to learn the partition\n",
    "def learn_partition(xx, yy, hyp, fixhyp, n_partition):\n",
    "    if \"decomp\" in fixhyp:\n",
    "        return fixhyp[\"decomp\"]\n",
    "\n",
    "    N_gibbs = 10\n",
    "    gibbs_iter = N_gibbs // 2\n",
    "    dim_limit = 3\n",
    "    maxNdata = 750\n",
    "\n",
    "    Nidx = min(maxNdata, xx.shape[0])\n",
    "    xx = xx[:Nidx]\n",
    "    yy = yy[:Nidx]\n",
    "\n",
    "    hyp_dirichlet = np.ones(n_partition)\n",
    "    prob_partition = hyp_dirichlet / hyp_dirichlet.sum()\n",
    "\n",
    "    z = fixhyp.get(\"z\", sample_categorical(prob_partition, xx.shape[1]))\n",
    "\n",
    "    z_best = z.copy()\n",
    "    minnlz = float('inf')\n",
    "\n",
    "    for i in range(N_gibbs):\n",
    "        for d in range(xx.shape[1]):\n",
    "            log_prob = np.full(n_partition, -np.inf)\n",
    "            nlz = np.full(n_partition, float('inf'))\n",
    "\n",
    "            for a in range(n_partition):\n",
    "                z[d] = a\n",
    "\n",
    "                if i >= gibbs_iter and np.sum(z == a) >= dim_limit:\n",
    "                    continue\n",
    "\n",
    "                nlz[a] = compute_nlz(xx, yy, hyp, z)\n",
    "                log_prob[a] = np.log(np.sum(z == a) + hyp_dirichlet[a]) - nlz[a]\n",
    "\n",
    "            z[d] = np.argmax(log_prob - np.log(-np.log(np.random.rand(n_partition))))\n",
    "\n",
    "            if minnlz > nlz[z[d]]:\n",
    "                z_best = z.copy()\n",
    "                minnlz = nlz[z[d]]\n",
    "\n",
    "    return z_best\n",
    "\n",
    "def compute_nlz(xx, yy, hyp, z):\n",
    "    \"\"\"\n",
    "    Compute the negative log likelihood (NLL) for the given decomposition.\n",
    "    \"\"\"\n",
    "    model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    model.train()\n",
    "    mll_value = -mll(model(model.train_inputs[0]), model.train_targets)\n",
    "    return mll_value.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gram(xx, hyp, z):\n",
    "    \"\"\"\n",
    "    Compute the Gram matrix for an additive Gaussian process (add-GP).\n",
    "\n",
    "    Parameters:\n",
    "    - xx: numpy.ndarray\n",
    "        Input data of shape (n_samples, n_features).\n",
    "    - hyp: dict\n",
    "        Dictionary containing hyperparameters:\n",
    "        - 'l': numpy.ndarray of shape (n_partitions, n_features): length scales.\n",
    "        - 'sigma': numpy.ndarray of shape (n_partitions,): signal variances.\n",
    "        - 'sigma0': numpy.ndarray of shape (n_partitions,): noise variances.\n",
    "    - hyp_idx: int\n",
    "        Index for the hyperparameter set to use.\n",
    "    - z: numpy.ndarray\n",
    "        Array of shape (n_features,) defining the decomposition of input dimensions.\n",
    "        Each element specifies the partition index for the corresponding feature.\n",
    "\n",
    "    Returns:\n",
    "    - K: numpy.ndarray\n",
    "        Gram matrix of shape (n_samples, n_samples).\n",
    "    \"\"\"\n",
    "    all_cat = np.unique(z)  # Unique partition indices in z\n",
    "    K = 0  # Initialize Gram matrix\n",
    "\n",
    "    for category in all_cat:\n",
    "        # Get indices of features belonging to the current partition\n",
    "        feature_indices = np.where(z == category)[0]\n",
    "        \n",
    "        # Extract relevant data and hyperparameters\n",
    "        xx_partition = xx[:, feature_indices]\n",
    "        l_partition = hyp['l'][feature_indices]\n",
    "        #print(f'This is hyp: {hyp}')\n",
    "        sigma_partition = hyp['sigma'] # TODO: Adapt so that each dimension can learn its own signal variance hyp['sigma'][category]\n",
    "        sigma0_partition = hyp['sigma0'] # TODO: Adapt so that each dimension can learn its own noise variance hyp['sigma0'][category]\n",
    "\n",
    "        # Compute the Gram matrix for this partition and accumulate\n",
    "        K += computeKmm(xx_partition, l_partition, sigma_partition, sigma0_partition)\n",
    "\n",
    "    return K\n",
    "\n",
    "def computeKmm(xx, l, sigma, sigma0):\n",
    "    \"\"\"\n",
    "    Compute the covariance (Gram) matrix for a single partition of features.\n",
    "\n",
    "    Parameters:\n",
    "    - xx: numpy.ndarray\n",
    "        Input data of shape (n_samples, n_features).\n",
    "    - l: numpy.ndarray\n",
    "        Length scales for the features.\n",
    "    - sigma: float\n",
    "        Signal variance.\n",
    "    - sigma0: float\n",
    "        Noise variance.\n",
    "\n",
    "    Returns:\n",
    "    - K: numpy.ndarray\n",
    "        Gram matrix of shape (n_samples, n_samples).\n",
    "    \"\"\"\n",
    "    # Compute squared distance matrix scaled by length scales\n",
    "    scaled_xx = (xx / l).numpy()\n",
    "    pairwise_sq_dists = np.sum(scaled_xx**2, axis=1, keepdims=True) - 2 * np.dot(scaled_xx, scaled_xx.T) + np.sum(scaled_xx**2, axis=1)\n",
    "    # Compute covariance matrix using the squared exponential kernel\n",
    "    K = sigma * np.exp(-0.5 * pairwise_sq_dists)\n",
    "\n",
    "    # Add noise variance on the diagonal\n",
    "    K += np.eye(K.shape[0]) * sigma0\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb_choose(xx, yy, kernel_matrix_inv, sigma0, sigma, l, xmin, xmax, kappa):\n",
    "    \"\"\"\n",
    "    Select the next evaluation point using the Upper Confidence Bound (UCB) acquisition function.\n",
    "\n",
    "    Parameters:\n",
    "    - xx: numpy.ndarray\n",
    "        Observed input points of shape (n_samples, n_features).\n",
    "    - yy: numpy.ndarray\n",
    "        Observed output values of shape (n_samples,).\n",
    "    - kernel_matrix_inv: numpy.ndarray\n",
    "        Precomputed inverse Gram matrix for the Gaussian process.\n",
    "    - sigma0: float\n",
    "        Noise variance.\n",
    "    - sigma: float\n",
    "        Signal variance.\n",
    "    - l: numpy.ndarray\n",
    "        Length scales of shape (n_features,).\n",
    "    - xmin: numpy.ndarray\n",
    "        Lower bounds of the search space, shape (n_features,).\n",
    "    - xmax: numpy.ndarray\n",
    "        Upper bounds of the search space, shape (n_features,).\n",
    "    - kappa: float\n",
    "        Scale factor for the confidence bound.\n",
    "\n",
    "    Returns:\n",
    "    - optimum: numpy.ndarray\n",
    "        The selected next evaluation point, shape (n_features,).\n",
    "    \"\"\"\n",
    "\n",
    "    def evaluate_ucb(x):\n",
    "        \"\"\"\n",
    "        Compute the UCB acquisition function value for a given point x.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: numpy.ndarray\n",
    "            A single input point of shape (n_features,).\n",
    "        \n",
    "        Returns:\n",
    "        - ucb: float\n",
    "            UCB acquisition function value.\n",
    "        \"\"\"\n",
    "        x = x.reshape(1, -1)  # Ensure x is 2D\n",
    "\n",
    "        # Compute kernel vector between x and observed data\n",
    "        k = compute_kernel_vector(x, xx, l, sigma).flatten()\n",
    "        yy_detached = yy.detach().numpy().flatten()\n",
    "\n",
    "        # Predictive mean and variance\n",
    "        mu = k.T @ kernel_matrix_inv @ yy_detached\n",
    "        sigma_sq = compute_kernel_scalar(x, l, sigma, sigma0) - k.T @ kernel_matrix_inv @ k\n",
    "\n",
    "        # UCB value\n",
    "        return -(mu + kappa * np.sqrt(max(sigma_sq, 0)))  # Negative for maximization\n",
    "\n",
    "    # Use guesses and observed data as initial candidates\n",
    "    best_ucb = float('-inf')\n",
    "    optimum = None\n",
    "\n",
    "    for x0 in xx:\n",
    "        # Constrain the optimization within bounds\n",
    "        bounds = [(xmin[i], xmax[i]) for i in range(len(xmin))]\n",
    "        # Optimize UCB starting from x0\n",
    "        res = minimize(evaluate_ucb, x0, bounds=bounds, method='L-BFGS-B', tol=1e-6)\n",
    "\n",
    "        #print(f'This is res: {res}')\n",
    "\n",
    "        if -res.fun > best_ucb:\n",
    "            best_ucb = -res.fun\n",
    "            optimum = res.x\n",
    "\n",
    "    return optimum\n",
    "\n",
    "def compute_kernel_vector(x, xx, l, sigma):\n",
    "    \"\"\"\n",
    "    Compute the kernel vector between a single point x and a dataset xx.\n",
    "\n",
    "    Parameters:\n",
    "    - x: numpy.ndarray\n",
    "        Single input point of shape (1, n_features).\n",
    "    - xx: numpy.ndarray\n",
    "        Dataset of shape (n_samples, n_features).\n",
    "    - l: numpy.ndarray\n",
    "        Length scales of shape (n_features,).\n",
    "    - sigma: float\n",
    "        Signal variance.\n",
    "\n",
    "    Returns:\n",
    "    - k: numpy.ndarray\n",
    "        Kernel vector of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    scaled_xx = (xx / l).numpy()\n",
    "    scaled_x = x / l\n",
    "    pairwise_sq_dists = np.sum(scaled_x**2, axis=1) - 2 * np.dot(scaled_x, scaled_xx.T) + np.sum(scaled_xx**2, axis=1)\n",
    "    return sigma * np.exp(-0.5 * pairwise_sq_dists)\n",
    "\n",
    "def compute_kernel_scalar(x, l, sigma, sigma0):\n",
    "    \"\"\"\n",
    "    Compute the kernel scalar (self-covariance) for a single point x.\n",
    "\n",
    "    Parameters:\n",
    "    - x: numpy.ndarray\n",
    "        Single input point of shape (1, n_features).\n",
    "    - l: numpy.ndarray\n",
    "        Length scales of shape (n_features,).\n",
    "    - sigma: float\n",
    "        Signal variance.\n",
    "    - sigma0: float\n",
    "        Noise variance.\n",
    "\n",
    "    Returns:\n",
    "    - k_scalar: float\n",
    "        Kernel scalar value.\n",
    "    \"\"\"\n",
    "    # Compute the squared norm of the scaled input\n",
    "    scaled_norm = np.sum((x / l) ** 2)\n",
    "\n",
    "    # Compute kernel scalar\n",
    "    k_scalar = sigma ** 2 * np.exp(-0.5 * scaled_norm) + sigma0 ** 2\n",
    "\n",
    "    return k_scalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gp2(syn, options=None):\n",
    "    \"\"\"\n",
    "    Maximize the function objective using Bayesian Optimization with additive Gaussian Processes.\n",
    "\n",
    "    Parameters:\n",
    "    - objective: callable, the function to optimize.\n",
    "    - xmin, xmax: array-like, the bounds of the search space.\n",
    "    - T: int, the number of sequential evaluations.\n",
    "    - initx, inity: array-like, initial observed inputs and outputs.\n",
    "    - options: dict, additional options.\n",
    "\n",
    "    Returns:\n",
    "    - results: dict containing inferred argmax points, function values, evaluated points, and timing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Default options\n",
    "    if options is None:\n",
    "        options = {}\n",
    "    options.setdefault('savefilenm', None)\n",
    "    options.setdefault('nK', 1)\n",
    "    options.setdefault('nFeatures', 10000)\n",
    "    options.setdefault('seed', 42)\n",
    "    options.setdefault('learn_interval', 10)\n",
    "    options.setdefault('n_iters', 200)\n",
    "    options.setdefault('learning rate', 1e-3)\n",
    "    options.setdefault('training iterations', 50)\n",
    "    options.setdefault('acq_f', 'UCB')\n",
    "\n",
    "    np.random.seed(options['seed'])\n",
    "\n",
    "    #TODO: Add start of BO using model savefilenm\n",
    "    t_start = 1\n",
    "    xx, yy = syn.simulate(49)\n",
    "    x_next, y_next = syn.simulate(1)\n",
    "    yy = yy.unsqueeze(-1)\n",
    "    y_next = y_next.unsqueeze(-1)\n",
    "    T = options['n_iters']\n",
    "    kappa = options['kappa']\n",
    "    train_time = []\n",
    "    choose_time = []\n",
    "    extra_time = []\n",
    "    predictions = []\n",
    "    losses = np.zeros((T, 1))\n",
    "    exploration = np.zeros(T)\n",
    "    exploitation = np.zeros(T)\n",
    "    mse = np.zeros((T, 1))\n",
    "\n",
    "\n",
    "    for t in range(t_start, T):\n",
    "\n",
    "        \n",
    "        # Innerloop for kernel learning\n",
    "        if t % options['learn_interval'] == 1:\n",
    "            start_time = time.time()\n",
    "            z, hyp = sample_struct_priors(xx, yy, options)\n",
    "            options['z'] = z\n",
    "            extra_time.append(time.time() - start_time)\n",
    "            print(f'Iter {t} - Decomposition of inputs: {z}')\n",
    "\n",
    "        #Training\n",
    "        start_time = time.time()\n",
    "        kernel_matrix = compute_gram(xx, hyp, z)\n",
    "        kernel_matrix_inv = np.linalg.inv(kernel_matrix)\n",
    "\n",
    "        all_cat = np.unique(z)\n",
    "        x_next = np.zeros_like(xx[0])\n",
    "        for cat in all_cat:\n",
    "            coords = z == cat\n",
    "            xx_sub = xx[:, coords]\n",
    "            xmin_sub = syn.lower_bounds[coords]\n",
    "            xmax_sub = syn.upper_bounds[coords]\n",
    "            l = hyp['l'][coords]\n",
    "            sigma = hyp['sigma']#[cat]\n",
    "            sigma0 = hyp['sigma0']#[cat]\n",
    "            beta = np.sqrt(len(xx_sub[0]) * np.log(2 * t) / 5)\n",
    "            optimum = ucb_choose(xx_sub, yy, kernel_matrix_inv, sigma0, sigma, l, xmin_sub, xmax_sub, beta)\n",
    "            x_next[coords] = optimum\n",
    "\n",
    "        train_time.append(time.time() -start_time)\n",
    "        x_next = torch.from_numpy(x_next)\n",
    "        y_next = syn.f.forward(x_next).unsqueeze(-1)\n",
    "        xx = torch.cat([xx, x_next.unsqueeze(0)], dim=0)\n",
    "        yy = torch.cat([yy, y_next.unsqueeze(0)], dim=0)\n",
    "\n",
    "        #Calculate metrics\n",
    "        predictions.append(y_next)\n",
    "        #with gpt_posterior_settings():\n",
    "        #    posterior= gp.posterior(xx)\n",
    "        #    y_preds = posterior.mean\n",
    "        #y_trues = syn.f.forward(xx).unsqueeze(-1)\n",
    "        #distances = (y_trues - y_preds)**2\n",
    "        #mse[t, 0] = distances.mean()\n",
    "        exploitation[t] = syn.f.forward(x_next) / syn.f.optimal_value\n",
    "        best_x = xx[torch.argmin(yy)]\n",
    "        exploration[t] = syn.f.forward(best_x) / syn.f.optimal_value\n",
    "        \n",
    "        if t % 5 == 0:\n",
    "            print(f'Iteration {t}: predicted {y_next.item()} at {x_next}')\n",
    "\n",
    "    results = {\n",
    "        'xx': xx,\n",
    "        'yy': yy,\n",
    "        'train_time': train_time,\n",
    "        'choose_time': choose_time,\n",
    "        'extra_time': extra_time,\n",
    "        #'loss': loss,\n",
    "        'predictions': predictions,\n",
    "        'exploration': exploration,\n",
    "        'exploitation': exploitation,\n",
    "        'mse': mse,\n",
    "\n",
    "    }\n",
    "\n",
    "    #save to logs\n",
    "    if options['savefilenm']:\n",
    "        \n",
    "        with open(options['savefilenm'], 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    synthetic = synthetic_datasets.SyntheticTestFun('ackley', 10, 0.0, negate=True)\n",
    "\n",
    "    options = {\n",
    "        \"savefilenm\": '../../output/experiments/structural_kernal_'+datetime.date.today().strftime(\"%y%m%d\")+'.pkl', # Placeholder for save file name\n",
    "        \"n_iters\": 200,\n",
    "        \"l\": 50,  # Length scale hyperparameter,\n",
    "        \"kappa\": 5,\n",
    "        \"acq_f\": \"UCB\",\n",
    "    }\n",
    "\n",
    "    #Start BO with Add-GP\n",
    "    return add_gp2(synthetic, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=InputDataWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=OptimizationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=BadInitialCandidatesWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPBO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
