{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import logsumexp\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import gpytorch\n",
    "import warnings\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.acquisition.analytic import LogExpectedImprovement, UpperConfidenceBound\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.models.utils import gpt_posterior_settings\n",
    "from botorch.models.transforms import Standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "from botorch.exceptions import OptimizationWarning, InputDataWarning, BadInitialCandidatesWarning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Adding modules\n",
    "sys.path.append(str(Path('./').resolve().parent.parent))\n",
    "from src.utils import synthetic_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OG paper conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Helper function to sample categorical data\n",
    "def sample_categorical(prob_partition, size):\n",
    "    return np.random.choice(len(prob_partition), size=size, p=prob_partition)\n",
    "\n",
    "# Main function\n",
    "def sample_struct_priors(xx, yy, fixhyp):\n",
    "    print(\"Start sample struct priors\")\n",
    "    dx = xx.shape[1]\n",
    "    n_partition = dx\n",
    "\n",
    "    hyp = {}\n",
    "\n",
    "    if all(k in fixhyp for k in [\"l\", \"sigma\", \"sigma0\"]):\n",
    "        hyp[\"l\"] = fixhyp[\"l\"]\n",
    "        hyp[\"sigma\"] = fixhyp[\"sigma\"]\n",
    "        hyp[\"sigma0\"] = fixhyp[\"sigma0\"]\n",
    "        decomp = learn_partition(xx, yy, hyp, fixhyp, n_partition)\n",
    "    else:\n",
    "        prob_partition = np.ones(n_partition) / n_partition\n",
    "        if \"z\" in fixhyp:\n",
    "            decomp = fixhyp[\"z\"]\n",
    "        else:\n",
    "            decomp = sample_categorical(prob_partition, dx)\n",
    "\n",
    "        num_iter = 2\n",
    "        guess_params = []\n",
    "\n",
    "        for _ in range(num_iter):\n",
    "            assert max(decomp) <= n_partition\n",
    "\n",
    "            def nll_func(params):\n",
    "                return compute_nlz_wrap(xx, yy, params, n_partition, decomp)\n",
    "\n",
    "            bounds = np.vstack([\n",
    "                np.tile([0, 10], (n_partition, 1)),\n",
    "                np.tile([-5, 2], (n_partition, 1)),\n",
    "                np.tile([-10, 1], (n_partition, 1))\n",
    "            ])\n",
    "\n",
    "            res = minimize(\n",
    "                nll_func, x0=np.zeros(bounds.shape[0]), bounds=bounds,\n",
    "                method='L-BFGS-B'\n",
    "            )\n",
    "\n",
    "            best_params = res.x\n",
    "            print(f\"Finished optimize hyp nll={res.fun}\")\n",
    "\n",
    "            guess_params.append(best_params)\n",
    "\n",
    "            l = np.exp(best_params[:n_partition][decomp])\n",
    "            sigma = np.exp(best_params[n_partition:2*n_partition])\n",
    "            sigma0 = np.exp(best_params[2*n_partition:])\n",
    "\n",
    "            hyp[\"l\"] = l\n",
    "            hyp[\"sigma\"] = sigma\n",
    "            hyp[\"sigma0\"] = sigma0\n",
    "\n",
    "            decomp = learn_partition(xx, yy, hyp, fixhyp, n_partition)\n",
    "            fixhyp[\"z\"] = decomp\n",
    "\n",
    "    return decomp, hyp\n",
    "\n",
    "# Helper function to learn the partition\n",
    "def learn_partition(xx, yy, hyp, fixhyp, n_partition):\n",
    "    if \"decomp\" in fixhyp:\n",
    "        return fixhyp[\"decomp\"]\n",
    "\n",
    "    N_gibbs = 10\n",
    "    gibbs_iter = N_gibbs // 2\n",
    "    dim_limit = 3\n",
    "    maxNdata = 750\n",
    "\n",
    "    Nidx = min(maxNdata, xx.shape[0])\n",
    "    xx = xx[:Nidx]\n",
    "    yy = yy[:Nidx]\n",
    "\n",
    "    hyp_dirichlet = np.ones(n_partition) * 1\n",
    "    prob_partition = hyp_dirichlet / hyp_dirichlet.sum()\n",
    "\n",
    "    if \"z\" in fixhyp:\n",
    "        z = fixhyp[\"z\"]\n",
    "    else:\n",
    "        z = sample_categorical(prob_partition, xx.shape[1])\n",
    "\n",
    "    z_best = None\n",
    "    minnlz = float('inf')\n",
    "\n",
    "    for i in range(N_gibbs):\n",
    "        for d in range(xx.shape[1]):\n",
    "            log_prob = np.full(n_partition, -np.inf)\n",
    "            nlz = np.full(n_partition, float('inf'))\n",
    "            \n",
    "            for a in range(n_partition):\n",
    "                z[d] = a\n",
    "\n",
    "                if i >= gibbs_iter and np.sum(z == a) >= dim_limit:\n",
    "                    continue\n",
    "\n",
    "                nlz[a] = compute_nlz(xx, yy, hyp, z)\n",
    "                log_prob[a] = np.log(np.sum(z == a) + hyp_dirichlet[a]) - nlz[a]\n",
    "\n",
    "            z[d] = np.argmax(log_prob - np.log(-np.log(np.random.rand(n_partition))))\n",
    "\n",
    "            if minnlz > nlz[z[d]]:\n",
    "                z_best = z.copy()\n",
    "                minnlz = nlz[z[d]]\n",
    "\n",
    "    return z_best\n",
    "\n",
    "# Placeholder for compute_nlz and compute_nlz_wrap\n",
    "def compute_nlz(xx, yy, hyp, z):\n",
    "    # Implement the calculation of the negative log likelihood here\n",
    "    return 0\n",
    "\n",
    "def compute_nlz_wrap(xx, yy, params, n_partition, decomp):\n",
    "    # Wrap the negative log likelihood computation\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gram(xx, hyp, hyp_idx, z):\n",
    "    \"\"\"\n",
    "    Compute the Gram matrix for an additive Gaussian process (add-GP).\n",
    "\n",
    "    Parameters:\n",
    "    - xx: numpy.ndarray\n",
    "        Input data of shape (n_samples, n_features).\n",
    "    - hyp: dict\n",
    "        Dictionary containing hyperparameters:\n",
    "        - 'l': numpy.ndarray of shape (n_partitions, n_features): length scales.\n",
    "        - 'sigma': numpy.ndarray of shape (n_partitions,): signal variances.\n",
    "        - 'sigma0': numpy.ndarray of shape (n_partitions,): noise variances.\n",
    "    - hyp_idx: int\n",
    "        Index for the hyperparameter set to use.\n",
    "    - z: numpy.ndarray\n",
    "        Array of shape (n_features,) defining the decomposition of input dimensions.\n",
    "        Each element specifies the partition index for the corresponding feature.\n",
    "\n",
    "    Returns:\n",
    "    - K: numpy.ndarray\n",
    "        Gram matrix of shape (n_samples, n_samples).\n",
    "    \"\"\"\n",
    "    all_cat = np.unique(z)  # Unique partition indices in z\n",
    "    K = 0  # Initialize Gram matrix\n",
    "\n",
    "    for category in all_cat:\n",
    "        # Get indices of features belonging to the current partition\n",
    "        feature_indices = np.where(z == category)[0]\n",
    "        \n",
    "        # Extract relevant data and hyperparameters\n",
    "        xx_partition = xx[:, feature_indices]\n",
    "        print(f'This is hyp: {hyp}')\n",
    "        l_partition = hyp['l'][hyp_idx, feature_indices]\n",
    "        sigma_partition = hyp['sigma'][hyp_idx, category]\n",
    "        sigma0_partition = hyp['sigma0'][hyp_idx, category]\n",
    "\n",
    "        # Compute the Gram matrix for this partition and accumulate\n",
    "        K += computeKmm(xx_partition, l_partition, sigma_partition, sigma0_partition)\n",
    "\n",
    "    return K\n",
    "\n",
    "def computeKmm(xx, l, sigma, sigma0):\n",
    "    \"\"\"\n",
    "    Compute the covariance (Gram) matrix for a single partition of features.\n",
    "\n",
    "    Parameters:\n",
    "    - xx: numpy.ndarray\n",
    "        Input data of shape (n_samples, n_features).\n",
    "    - l: numpy.ndarray\n",
    "        Length scales for the features.\n",
    "    - sigma: float\n",
    "        Signal variance.\n",
    "    - sigma0: float\n",
    "        Noise variance.\n",
    "\n",
    "    Returns:\n",
    "    - K: numpy.ndarray\n",
    "        Gram matrix of shape (n_samples, n_samples).\n",
    "    \"\"\"\n",
    "    # Compute squared distance matrix scaled by length scales\n",
    "    scaled_xx = xx / l\n",
    "    pairwise_sq_dists = np.sum(scaled_xx**2, axis=1, keepdims=True) - 2 * np.dot(scaled_xx, scaled_xx.T) + np.sum(scaled_xx**2, axis=1)\n",
    "\n",
    "    # Compute covariance matrix using the squared exponential kernel\n",
    "    K = sigma * np.exp(-0.5 * pairwise_sq_dists)\n",
    "\n",
    "    # Add noise variance on the diagonal\n",
    "    K += np.eye(K.shape[0]) * sigma0\n",
    "\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ucb_choose(xx, yy, kernel_matrix_inv, guesses, sigma0, sigma, l, xmin, xmax, alpha, beta):\n",
    "    \"\"\"\n",
    "    Select the next evaluation point using the Upper Confidence Bound (UCB) acquisition function.\n",
    "\n",
    "    Parameters:\n",
    "    - xx: numpy.ndarray\n",
    "        Observed input points of shape (n_samples, n_features).\n",
    "    - yy: numpy.ndarray\n",
    "        Observed output values of shape (n_samples,).\n",
    "    - kernel_matrix_inv: numpy.ndarray\n",
    "        Precomputed inverse Gram matrix for the Gaussian process.\n",
    "    - guesses: numpy.ndarray\n",
    "        Points to consider for evaluation, shape (n_guesses, n_features).\n",
    "    - sigma0: float\n",
    "        Noise variance.\n",
    "    - sigma: float\n",
    "        Signal variance.\n",
    "    - l: numpy.ndarray\n",
    "        Length scales of shape (n_features,).\n",
    "    - xmin: numpy.ndarray\n",
    "        Lower bounds of the search space, shape (n_features,).\n",
    "    - xmax: numpy.ndarray\n",
    "        Upper bounds of the search space, shape (n_features,).\n",
    "    - alpha: float\n",
    "        Exploration-exploitation balance parameter.\n",
    "    - beta: float\n",
    "        Scale factor for the confidence bound.\n",
    "\n",
    "    Returns:\n",
    "    - optimum: numpy.ndarray\n",
    "        The selected next evaluation point, shape (n_features,).\n",
    "    \"\"\"\n",
    "\n",
    "    def evaluate_ucb(x):\n",
    "        \"\"\"\n",
    "        Compute the UCB acquisition function value for a given point x.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: numpy.ndarray\n",
    "            A single input point of shape (n_features,).\n",
    "        \n",
    "        Returns:\n",
    "        - ucb: float\n",
    "            UCB acquisition function value.\n",
    "        \"\"\"\n",
    "        x = x.reshape(1, -1)  # Ensure x is 2D\n",
    "\n",
    "        # Compute kernel vector between x and observed data\n",
    "        k = compute_kernel_vector(x, xx, l, sigma)\n",
    "\n",
    "        # Predictive mean and variance\n",
    "        mu = k.T @ kernel_matrix_inv @ yy\n",
    "        sigma_sq = compute_kernel_scalar(x, l, sigma, sigma0) - k.T @ kernel_matrix_inv @ k\n",
    "\n",
    "        # UCB value\n",
    "        return -(mu + beta * np.sqrt(max(sigma_sq, 0)))  # Negative for maximization\n",
    "\n",
    "    # Use guesses and observed data as initial candidates\n",
    "    initial_points = np.vstack([guesses, xx])\n",
    "    best_ucb = float('-inf')\n",
    "    optimum = None\n",
    "\n",
    "    for x0 in initial_points:\n",
    "        # Constrain the optimization within bounds\n",
    "        bounds = [(xmin[i], xmax[i]) for i in range(len(xmin))]\n",
    "\n",
    "        # Optimize UCB starting from x0\n",
    "        res = minimize(evaluate_ucb, x0, bounds=bounds, method='L-BFGS-B')\n",
    "\n",
    "        if -res.fun > best_ucb:\n",
    "            best_ucb = -res.fun\n",
    "            optimum = res.x\n",
    "\n",
    "    return optimum\n",
    "\n",
    "\n",
    "def compute_kernel_vector(x, xx, l, sigma):\n",
    "    \"\"\"\n",
    "    Compute the kernel vector between a single point x and a dataset xx.\n",
    "\n",
    "    Parameters:\n",
    "    - x: numpy.ndarray\n",
    "        Single input point of shape (1, n_features).\n",
    "    - xx: numpy.ndarray\n",
    "        Dataset of shape (n_samples, n_features).\n",
    "    - l: numpy.ndarray\n",
    "        Length scales of shape (n_features,).\n",
    "    - sigma: float\n",
    "        Signal variance.\n",
    "\n",
    "    Returns:\n",
    "    - k: numpy.ndarray\n",
    "        Kernel vector of shape (n_samples,).\n",
    "    \"\"\"\n",
    "    scaled_xx = xx / l\n",
    "    scaled_x = x / l\n",
    "    pairwise_sq_dists = np.sum(scaled_x**2, axis=1) - 2 * np.dot(scaled_x, scaled_xx.T) + np.sum(scaled_xx**2, axis=1)\n",
    "    return sigma * np.exp(-0.5 * pairwise_sq_dists)\n",
    "\n",
    "\n",
    "def compute_kernel_scalar(x, l, sigma, sigma0):\n",
    "    \"\"\"\n",
    "    Compute the kernel scalar (self-covariance) for a single point x.\n",
    "\n",
    "    Parameters:\n",
    "    - x: numpy.ndarray\n",
    "        Single input point of shape (1, n_features).\n",
    "    - l: numpy.ndarray\n",
    "        Length scales of shape (n_features,).\n",
    "    - sigma: float\n",
    "        Signal variance.\n",
    "    - sigma0: float\n",
    "        Noise variance.\n",
    "\n",
    "    Returns:\n",
    "    - k_scalar: float\n",
    "        Kernel scalar value.\n",
    "    \"\"\"\n",
    "    return sigma + sigma0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_gpopt(objective, xmin, xmax, T, initx=None, inity=None, options=None):\n",
    "    \"\"\"\n",
    "    Maximize the function objective using Bayesian Optimization with additive Gaussian Processes.\n",
    "\n",
    "    Parameters:\n",
    "    - objective: callable, the function to optimize.\n",
    "    - xmin, xmax: array-like, the bounds of the search space.\n",
    "    - T: int, the number of sequential evaluations.\n",
    "    - initx, inity: array-like, initial observed inputs and outputs.\n",
    "    - options: dict, additional options.\n",
    "\n",
    "    Returns:\n",
    "    - results: dict containing inferred argmax points, function values, evaluated points, and timing.\n",
    "    \"\"\"\n",
    "    # Default options\n",
    "    if options is None:\n",
    "        options = {}\n",
    "    options.setdefault('restart', 0)\n",
    "    options.setdefault('savefilenm', None)\n",
    "    options.setdefault('noiselevel', 0)\n",
    "    options.setdefault('nK', 1)\n",
    "    options.setdefault('nFeatures', 10000)\n",
    "    options.setdefault('seed', 42)\n",
    "    options.setdefault('learn_interval', 10)\n",
    "\n",
    "    np.random.seed(options['seed'])\n",
    "\n",
    "    if options['restart'] and options['savefilenm'] and os.path.exists(options['savefilenm']):\n",
    "        with open(options['savefilenm'], 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        xx = results['xx']\n",
    "        yy = results['yy']\n",
    "        choose_time = results['choose_time']\n",
    "        extra_time = results['extra_time']\n",
    "        t_start = results['t']\n",
    "        z = results['z']\n",
    "    else:\n",
    "        t_start = 0\n",
    "        if initx is None or inity is None:\n",
    "            initx = np.random.uniform(xmin, xmax, (1, len(xmin)))\n",
    "            inity = objective(initx)\n",
    "\n",
    "        xx = initx\n",
    "        yy = inity\n",
    "        choose_time = []\n",
    "        extra_time = []\n",
    "\n",
    "    for t in range(t_start + 1, T + 1):\n",
    "        # Learn structure\n",
    "        if t % options['learn_interval'] == 1:\n",
    "            start_time = time.time()\n",
    "            z, hyp = sample_struct_priors(xx, yy, options)\n",
    "            options['z'] = z\n",
    "            extra_time.append(time.time() - start_time)\n",
    "\n",
    "        # Choose next point\n",
    "        start_time = time.time()\n",
    "        kernel_matrix = compute_gram(xx, hyp, 1, z)\n",
    "        kernel_matrix_inv = np.linalg.inv(kernel_matrix)\n",
    "\n",
    "        x_next = np.zeros_like(xx[0])\n",
    "        all_categories = np.unique(z)\n",
    "\n",
    "        for cat in all_categories:\n",
    "            coords = z == cat\n",
    "            xx_sub = xx[:, coords]\n",
    "            xmin_sub = xmin[coords]\n",
    "            xmax_sub = xmax[coords]\n",
    "            l = hyp['l'][:, coords]\n",
    "            sigma = hyp['sigma'][cat]\n",
    "            sigma0 = hyp['sigma0'][cat]\n",
    "            alpha = 1\n",
    "            beta = np.sqrt(len(xx_sub[0]) * np.log(2 * t) / 5)\n",
    "\n",
    "            optimum = ucb_choose(xx_sub, yy, kernel_matrix_inv, sigma0, sigma, l, xmin_sub, xmax_sub, alpha, beta)\n",
    "            x_next[coords] = optimum\n",
    "\n",
    "        choose_time.append(time.time() - start_time)\n",
    "\n",
    "        xx = np.vstack([xx, x_next])\n",
    "        yy = np.vstack([yy, objective(x_next) + np.random.normal(0, options['noiselevel'], size=(1,))])\n",
    "\n",
    "        print(f\"{t}: val={yy[-1][0]}\")\n",
    "\n",
    "        if options['savefilenm'] and t % 10 == 0:\n",
    "            results = {\n",
    "                'xx': xx,\n",
    "                'yy': yy,\n",
    "                'choose_time': choose_time,\n",
    "                'extra_time': extra_time,\n",
    "                't': t,\n",
    "                'z': z\n",
    "            }\n",
    "            with open(options['savefilenm'], 'wb') as f:\n",
    "                pickle.dump(results, f)\n",
    "\n",
    "    return {\n",
    "        'xx': xx,\n",
    "        'yy': yy,\n",
    "        'choose_time': choose_time,\n",
    "        'extra_time': extra_time\n",
    "    }\n",
    "\n",
    "# Supporting functions (e.g., sample_struct_priors, compute_gram, ucb_choose) should also be implemented.\n",
    "# These functions involve translating the specific mathematical operations in MATLAB to Python using NumPy or SciPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sample struct priors\n",
      "This is hyp: {'l': array([50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50., 50.,\n",
      "       50., 50., 50., 50., 50., 50., 50.]), 'sigma': array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.,\n",
      "       5., 5., 5.]), 'sigma0': array([0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "       0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001,\n",
      "       0.0001, 0.0001, 0.0001, 0.0001])}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     add_gpopt(f, xmin, xmax, \u001b[38;5;241m200\u001b[39m,options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 43\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m options \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msavefilenm\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Placeholder for save file name\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnK\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m,  \u001b[38;5;66;03m# Number of maximums to sample\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigma0\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mones(dx) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.0001\u001b[39m,  \u001b[38;5;66;03m# Noise variance\u001b[39;00m\n\u001b[0;32m     40\u001b[0m }\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Start Bayesian Optimization with Add-GP\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43madd_gpopt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 58\u001b[0m, in \u001b[0;36madd_gpopt\u001b[1;34m(objective, xmin, xmax, T, initx, inity, options)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Choose next point\u001b[39;00m\n\u001b[0;32m     57\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 58\u001b[0m kernel_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m kernel_matrix_inv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(kernel_matrix)\n\u001b[0;32m     61\u001b[0m x_next \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(xx[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[15], line 33\u001b[0m, in \u001b[0;36mcompute_gram\u001b[1;34m(xx, hyp, hyp_idx, z)\u001b[0m\n\u001b[0;32m     31\u001b[0m xx_partition \u001b[38;5;241m=\u001b[39m xx[:, feature_indices]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is hyp: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m l_partition \u001b[38;5;241m=\u001b[39m \u001b[43mhyp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhyp_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     34\u001b[0m sigma_partition \u001b[38;5;241m=\u001b[39m hyp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m'\u001b[39m][hyp_idx, category]\n\u001b[0;32m     35\u001b[0m sigma0_partition \u001b[38;5;241m=\u001b[39m hyp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma0\u001b[39m\u001b[38;5;124m'\u001b[39m][hyp_idx, category]\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define function to sample from additive Gaussian Process (add-GP)\n",
    "def sample_addGP(dx, n_samples, xmin, xmax):\n",
    "    \"\"\"\n",
    "    Placeholder for the sample_addGP function. This should sample a function\n",
    "    from an additive Gaussian Process.\n",
    "\n",
    "    Parameters:\n",
    "    - dx: int, number of dimensions.\n",
    "    - n_samples: int, number of samples to generate.\n",
    "    - xmin: numpy.ndarray, lower bounds of the input space.\n",
    "    - xmax: numpy.ndarray, upper bounds of the input space.\n",
    "\n",
    "    Returns:\n",
    "    - function: Callable that takes an input and evaluates the sampled add-GP.\n",
    "    \"\"\"\n",
    "    # Placeholder implementation (replace with your actual function)\n",
    "    def sampled_function(x):\n",
    "        return np.sum(np.sin(x), axis=-1)\n",
    "\n",
    "    return sampled_function\n",
    "\n",
    "\n",
    "# Define main script logic\n",
    "def main():\n",
    "    # Set parameters\n",
    "    dx = 20\n",
    "    xmin = np.zeros(dx)\n",
    "    xmax = np.ones(dx)\n",
    "\n",
    "    # Sample function from add-GP\n",
    "    f = sample_addGP(dx, dx, xmin, xmax)\n",
    "\n",
    "    # Set options\n",
    "    options = {\n",
    "        \"savefilenm\": None,  # Placeholder for save file name\n",
    "        \"nK\": 5,  # Number of maximums to sample\n",
    "        \"l\": np.ones(dx) * 50,  # Length scale hyperparameter\n",
    "        \"sigma\": np.ones(dx) * 5,  # Signal variance\n",
    "        \"sigma0\": np.ones(dx) * 0.0001,  # Noise variance\n",
    "    }\n",
    "\n",
    "    # Start Bayesian Optimization with Add-GP\n",
    "    add_gpopt(f, xmin, xmax, 200,options=options)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel):\n",
    "        super(GP, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to sample categorical data\n",
    "def sample_categorical(prob_partition, size):\n",
    "    return np.random.choice(len(prob_partition), size=size, p=prob_partition)\n",
    "\n",
    "# Main function\n",
    "def sample_struct_priors(xx, yy, fixhyp):\n",
    "    print(\"Start sample struct priors\")\n",
    "    dx = xx.shape[1]\n",
    "    n_partition = dx\n",
    "\n",
    "    hyp = {}\n",
    "\n",
    "    if all(k in fixhyp for k in [\"l\", \"sigma\", \"sigma0\"]):\n",
    "        hyp[\"l\"] = fixhyp[\"l\"]\n",
    "        hyp[\"sigma\"] = fixhyp[\"sigma\"]\n",
    "        hyp[\"sigma0\"] = fixhyp[\"sigma0\"]\n",
    "        decomp = learn_partition(xx, yy, hyp, fixhyp, n_partition)\n",
    "    else:\n",
    "        prob_partition = np.ones(n_partition) / n_partition\n",
    "        decomp = fixhyp.get(\"z\", sample_categorical(prob_partition, dx))\n",
    "\n",
    "        num_iter = 2\n",
    "        for _ in range(num_iter):\n",
    "            assert max(decomp) <= n_partition\n",
    "\n",
    "            def nll_func(params):\n",
    "                return compute_nlz_wrap(xx, yy, params, n_partition, decomp)\n",
    "\n",
    "            bounds = np.vstack([\n",
    "                np.tile([0, 10], (n_partition, 1)),\n",
    "                np.tile([-5, 2], (n_partition, 1)),\n",
    "                np.tile([-10, 1], (n_partition, 1))\n",
    "            ])\n",
    "\n",
    "            res = minimize(\n",
    "                nll_func, x0=np.zeros(bounds.shape[0]), bounds=bounds,\n",
    "                method='L-BFGS-B'\n",
    "            )\n",
    "\n",
    "            best_params = res.x\n",
    "            #print(f\"Finished optimize hyp nll={res.fun}\")\n",
    "\n",
    "            l = np.exp(best_params[:n_partition][decomp])\n",
    "            sigma = np.exp(best_params[n_partition:2*n_partition])\n",
    "            sigma0 = np.exp(best_params[2*n_partition:])\n",
    "\n",
    "            hyp[\"l\"] = l\n",
    "            hyp[\"sigma\"] = sigma\n",
    "            hyp[\"sigma0\"] = sigma0\n",
    "\n",
    "            decomp = learn_partition(xx, yy, hyp, fixhyp, n_partition)\n",
    "            fixhyp[\"z\"] = decomp\n",
    "\n",
    "    return decomp, hyp\n",
    "\n",
    "# Helper function to learn the partition\n",
    "def learn_partition(xx, yy, hyp, fixhyp, n_partition):\n",
    "    if \"decomp\" in fixhyp:\n",
    "        return fixhyp[\"decomp\"]\n",
    "\n",
    "    N_gibbs = 10\n",
    "    gibbs_iter = N_gibbs // 2\n",
    "    dim_limit = 3\n",
    "    maxNdata = 750\n",
    "\n",
    "    Nidx = min(maxNdata, xx.shape[0])\n",
    "    xx = xx[:Nidx]\n",
    "    yy = yy[:Nidx]\n",
    "\n",
    "    hyp_dirichlet = np.ones(n_partition)\n",
    "    prob_partition = hyp_dirichlet / hyp_dirichlet.sum()\n",
    "\n",
    "    z = fixhyp.get(\"z\", sample_categorical(prob_partition, xx.shape[1]))\n",
    "\n",
    "    z_best = z.copy()\n",
    "    minnlz = float('inf')\n",
    "\n",
    "    for i in range(N_gibbs):\n",
    "        for d in range(xx.shape[1]):\n",
    "            log_prob = np.full(n_partition, -np.inf)\n",
    "            nlz = np.full(n_partition, float('inf'))\n",
    "\n",
    "            for a in range(n_partition):\n",
    "                z[d] = a\n",
    "\n",
    "                if i >= gibbs_iter and np.sum(z == a) >= dim_limit:\n",
    "                    continue\n",
    "\n",
    "                nlz[a] = compute_nlz(xx, yy, hyp, z)\n",
    "                log_prob[a] = np.log(np.sum(z == a) + hyp_dirichlet[a]) - nlz[a]\n",
    "\n",
    "            z[d] = np.argmax(log_prob - np.log(-np.log(np.random.rand(n_partition))))\n",
    "\n",
    "            if minnlz > nlz[z[d]]:\n",
    "                z_best = z.copy()\n",
    "                minnlz = nlz[z[d]]\n",
    "\n",
    "    return z_best\n",
    "\n",
    "# Placeholder for compute_nlz and compute_nlz_wrap\n",
    "def compute_nlz(xx, yy, hyp, z):\n",
    "    \"\"\"\n",
    "    Compute the negative log likelihood (NLL) for the given decomposition.\n",
    "    Use a Gaussian Process (GP) model from BoTorch for evaluation.\n",
    "    \"\"\"\n",
    "    model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    model.train()\n",
    "    mll_value = -mll(model(model.train_inputs[0]), model.train_targets)\n",
    "    return mll_value.item()\n",
    "\n",
    "def compute_nlz_wrap(xx, yy, params, n_partition, decomp):\n",
    "    \"\"\"\n",
    "    Wrap the NLL computation for parameter optimization.\n",
    "    \"\"\"\n",
    "    hyp = {\n",
    "        \"l\": np.exp(params[:n_partition]),\n",
    "        \"sigma\": np.exp(params[n_partition:2*n_partition]),\n",
    "        \"sigma0\": np.exp(params[2*n_partition:])\n",
    "    }\n",
    "    return compute_nlz(xx, yy, hyp, decomp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gp2(syn, options=None):\n",
    "    \"\"\"\n",
    "    Maximize the function objective using Bayesian Optimization with additive Gaussian Processes.\n",
    "\n",
    "    Parameters:\n",
    "    - objective: callable, the function to optimize.\n",
    "    - xmin, xmax: array-like, the bounds of the search space.\n",
    "    - T: int, the number of sequential evaluations.\n",
    "    - initx, inity: array-like, initial observed inputs and outputs.\n",
    "    - options: dict, additional options.\n",
    "\n",
    "    Returns:\n",
    "    - results: dict containing inferred argmax points, function values, evaluated points, and timing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Default options\n",
    "    if options is None:\n",
    "        options = {}\n",
    "    options.setdefault('savefilenm', None)\n",
    "    options.setdefault('nK', 1)\n",
    "    options.setdefault('nFeatures', 10000)\n",
    "    options.setdefault('seed', 42)\n",
    "    options.setdefault('learn_interval', 10)\n",
    "    options.setdefault('n_iters', 200)\n",
    "    options.setdefault('learning rate', 1e-3)\n",
    "    options.setdefault('training iterations', 50)\n",
    "    options.setdefault('acq_f', 'UCB')\n",
    "\n",
    "    np.random.seed(options['seed'])\n",
    "\n",
    "    #TODO: Add start of BO using model savefilenm\n",
    "    t_start = 0\n",
    "    xx, yy = syn.simulate(49)\n",
    "    x_next, y_next = syn.simulate(1)\n",
    "    yy = yy.unsqueeze(-1)\n",
    "    y_next = y_next.unsqueeze(-1)\n",
    "    bds = torch.tensor(np.stack([syn.lower_bounds, syn.upper_bounds]))\n",
    "    T = options['n_iters']\n",
    "    kappa = options['kappa']\n",
    "    train_time = []\n",
    "    choose_time = []\n",
    "    extra_time = []\n",
    "    predictions = []\n",
    "    losses = np.zeros((T, 1))\n",
    "    exploration = np.zeros(T)\n",
    "    exploitation = np.zeros(T)\n",
    "    mse = np.zeros((T, 1))\n",
    "\n",
    "\n",
    "    for t in range(t_start, T):\n",
    "\n",
    "\n",
    "        #Training\n",
    "        start_time = time.time()\n",
    "        gp = SingleTaskGP(xx, yy)\n",
    "        gp.train()\n",
    "        optimizer = torch.optim.Adam(gp.parameters(), lr=options['learning rate'])\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "        l = 0.0\n",
    "        for _ in range(options['training iterations']):\n",
    "            optimizer.zero_grad()\n",
    "            output = gp(gp.train_inputs[0])\n",
    "            loss = -mll(output, gp.train_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            l += loss \n",
    "        losses[t, 0] = l / options['training iterations']\n",
    "        train_time.append(time.time() -start_time)\n",
    "\n",
    "\n",
    "\n",
    "        #Choosing new query\n",
    "        start_time = time.time()\n",
    "        if options['acq_f'] == 'UCB':\n",
    "            acq = UpperConfidenceBound(model=gp, beta=kappa, maximize=False)\n",
    "        elif options['acq_f'] == 'EI':\n",
    "            acq = LogExpectedImprovement(model=gp, best_f=yy.min(), maximize=False)\n",
    "\n",
    "        x_next, _ = optimize_acqf(\n",
    "            acq_function=acq,\n",
    "            bounds=bds,\n",
    "            q=1,\n",
    "            num_restarts=20,\n",
    "            raw_samples=200,\n",
    "        )\n",
    "        y_next = syn.f.forward(x_next).unsqueeze(-1)\n",
    "        xx = torch.cat([xx, x_next], dim=0)\n",
    "        yy = torch.cat([yy, y_next], dim=0)\n",
    "        choose_time.append(time.time() - start_time)\n",
    "\n",
    "\n",
    "\n",
    "        # Innerloop for kernel learning\n",
    "        if t % options['learn_interval'] == 1:\n",
    "            start_time = time.time()\n",
    "            z, hyp = sample_struct_priors(xx, yy, options)\n",
    "            options['z'] = z\n",
    "            extra_time.append(time.time() - start_time)\n",
    "\n",
    "\n",
    "\n",
    "        #Calculate metrics\n",
    "        predictions.append(y_next)\n",
    "        with gpt_posterior_settings():\n",
    "            posterior= gp.posterior(xx)\n",
    "            y_preds = posterior.mean\n",
    "        y_trues = syn.f.forward(xx).unsqueeze(-1)\n",
    "        distances = (y_trues - y_preds)**2\n",
    "        mse[t, 0] = distances.mean()\n",
    "        exploitation[t] = syn.f.forward(x_next) / syn.f.optimal_value\n",
    "        best_x = xx[torch.argmin(y_preds)]\n",
    "        exploration[t] = syn.f.forward(best_x) / syn.f.optimal_value\n",
    "        \n",
    "\n",
    "        #save to logs\n",
    "        if options['savefilenm'] and t % 10 == 0 and 2 == 3: ##TODO: Remove this false statement\n",
    "            results = {\n",
    "                'xx': xx,\n",
    "                'yy': yy,\n",
    "                'choose_time': choose_time,\n",
    "                'extra_time': extra_time,\n",
    "                't': t,\n",
    "                'z': z\n",
    "            }\n",
    "            with open(options['savefilenm'], 'wb') as f:\n",
    "                pickle.dump(results, f)\n",
    "\n",
    "    results = {\n",
    "        'xx': xx,\n",
    "        'yy': yy,\n",
    "        'train_time': train_time,\n",
    "        'choose_time': choose_time,\n",
    "        'extra_time': extra_time,\n",
    "        'loss': loss,\n",
    "        'predictions': predictions,\n",
    "        'exploration': exploration,\n",
    "        'exploitation': exploitation,\n",
    "        'maes': mse,\n",
    "        'model': gp,\n",
    "\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    synthetic = synthetic_datasets.SyntheticTestFun('ackley', 10, 0.0)\n",
    "\n",
    "    options = {\n",
    "        \"savefilenm\": None, #\"../../output\" # Placeholder for save file name\n",
    "        \"n_iters\": 200,\n",
    "        \"l\": 50,  # Length scale hyperparameter,\n",
    "        \"kappa\": 5,\n",
    "        \"acq_f\": \"UCB\",\n",
    "    }\n",
    "\n",
    "    #Start BO with Add-GP\n",
    "    return add_gp2(synthetic, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=InputDataWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.8358084884941872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.8358084884941872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.7674709956241843\n",
      "Finished optimize hyp nll=1.7674709956241843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.718383500745665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.718383500745665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.6814163749729965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.6814163749729965\n",
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.6525738922273185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.6525738922273185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.6294427922035426\n",
      "Finished optimize hyp nll=1.6294427922035426\n",
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.6104794579497723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.6104794579497723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.5946505591098998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.5946505591098998\n",
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.5812382860470606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.5812382860470606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.5697284630640758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.5697284630640758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.5597431199463991\n",
      "Finished optimize hyp nll=1.5597431199463991\n",
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.5509981921228426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.5509981921228426\n",
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.5432760628752449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.5432760628752449\n",
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.5364072075774304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.5364072075774304\n",
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.5302576041434046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.5302576041434046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.5247199015485748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.5247199015485748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.5197070996736421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.5197070996736421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.515147945027176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.515147945027176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.5109835223846866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.5109835223846866\n",
      "Start sample struct priors\n",
      "Finished optimize hyp nll=1.507164694982286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_8652\\4091692254.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = SingleTaskGP(torch.tensor(xx), torch.tensor(yy), outcome_transform=Standardize(m=1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished optimize hyp nll=1.507164694982286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mauri\\miniconda3\\envs\\GPBO\\Lib\\site-packages\\botorch\\optim\\initializers.py:433: BadInitialCandidatesWarning: Unable to find non-zero acquisition function values - initial conditions are being selected randomly.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPBO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
